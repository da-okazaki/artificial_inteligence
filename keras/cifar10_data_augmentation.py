# -*- coding: utf-8 -*-
"""Colaboratory へようこそ

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/welcome.ipynb
"""

import numpy as np
import matplotlib.pyplot as plt
from keras import backend as K
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.datasets import cifar10
from keras.utils import to_categorical

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

X_train.shape

#0以上1以下の値にしてから，Kerasのデータ形式に変換
x_train, x_test = X_train / 255.0, X_test / 255.0

#Kerasのto_categorical 関数を用いてラベルを hot-one 表現に変換
y_train, y_test = to_categorical(y_train), to_categorical(y_test)

i = 0
plt.imshow(image.array_to_img(x_train[i], scale=True))

def show_imgs(imgs):
  # imgs: 表示させる画像のデータセット 
  imgs_num = len(imgs) #画像の数
  row_num = np.ceil(imgs_num/4) #行数
  plt.figure(figsize=(4, row_num), dpi=32*2)
  for i, img in enumerate(imgs):
    plt.subplot(row_num, 4, i+1)
    plt.tick_params(labelbottom="off") # x軸の削除
    plt.tick_params(labelleft="off") # y軸の削除
    plt.imshow(image.array_to_img(img, scale=True))

show_imgs(x_train[0:20])


datagen = ImageDataGenerator(width_shift_range=0.5)
x = datagen.flow(x_train, batch_size=10)[0]
y = datagen.flow(x_train, batch_size=10)[0]

print(x.shape, y.shape)
show_imgs([x[0],y[0]])

max_img_num = 12
imgs = []
for img in datagen.flow(x_train[1:2], batch_size=1):
    imgs.append(img[0])
    if len(imgs) >= max_img_num: 
      break
show_imgs(imgs)

datagen = ImageDataGenerator(height_shift_range=0.5)
max_img_num = 12
imgs = []
for img in datagen.flow(x_train[1:2], batch_size=1):
    imgs.append(img[0])
    if len(imgs) >= max_img_num: 
      break
show_imgs(imgs)

datagen = ImageDataGenerator(zoom_range=0.5)
max_img_num = 12
imgs = []
for img in datagen.flow(x_train[1:2], batch_size=1):
    imgs.append(img[0])
    if len(imgs) >= max_img_num: 
      break
show_imgs(imgs)

datagen = ImageDataGenerator(shear_range=20.0)
max_img_num = 12
imgs = []
for img in datagen.flow(x_train[1:2], batch_size=1):
    imgs.append(img[0])
    if len(imgs) >= max_img_num: 
      break
show_imgs(imgs)

datagen = ImageDataGenerator(horizontal_flip = True, vertical_flip = True)
max_img_num = 12
imgs = []
for img in datagen.flow(x_train[1:2], batch_size=1):
    imgs.append(img[0])
    if len(imgs) >= max_img_num: 
      break
show_imgs(imgs)

!git clone https://github.com/yu4u/cutout-random-erasing ./random_eraser 


import sys
sys.path.append("/content/random_eraser/") 

from random_eraser import get_random_eraser

datagen = ImageDataGenerator(
            preprocessing_function = get_random_eraser(p=0.5, s_l=0.02, s_h=0.2, r_1=0.3, r_2=1/0.3, v_l=0, v_h=0))

max_img_num = 12
imgs = []
for img in datagen.flow(x_train[1:2], batch_size=1):
    imgs.append(img[0])

    if len(imgs) >= max_img_num: 
      break
show_imgs(imgs)

!git clone https://github.com/yu4u/cutout-random-erasing ./random_eraser 

import sys
sys.path.append("/content/random_eraser/")

import tensorflow as tf
import numpy as np
from keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dense, Dropout, Flatten
from keras.models import Sequential

def CNN(input_shape=(32, 32, 3), class_num=10):
  model = Sequential()
  model.add(Conv2D(16, (3, 3), padding='same', kernel_initializer='he_normal', input_shape=input_shape))
  model.add(Activation('relu'))
  model.add(Conv2D(16, (3, 3), padding='same', kernel_initializer='he_normal'))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))


  model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='he_normal'))
  model.add(Activation('relu'))
  model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='he_normal'))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))

  model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal'))
  model.add(Activation('relu'))
  model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal'))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))

  model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal'))
  model.add(Activation('relu'))
  model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal'))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  
  model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal'))
  model.add(Activation('relu'))
  model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal'))
  model.add(BatchNormalization())
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))


  model.add(Flatten())
  model.add(Dense(512, activation='relu', kernel_initializer='he_normal'))
  model.add(Dropout(rate=0.5))
  model.add(Dense(class_num, kernel_initializer='he_normal'))
  model.add(Activation('softmax'))
  return model

import keras.backend as K

K.clear_session()

from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator 
from random_eraser import get_random_eraser
import os
from keras.optimizers import Adam


# Cifar-10 の読み込み
(X_train, y_train), (X_test, y_test) = cifar10.load_data()
x_train, x_test = (X_train / 255.0), (X_test / 255.0)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)  

# データ増大の設定
datagen = ImageDataGenerator(
            featurewise_center=False,
            samplewise_center=False,
            featurewise_std_normalization=False,
            samplewise_std_normalization=False,
            zca_whitening=False,
            zca_epsilon=1e-06,
            rotation_range=15,
            
            width_shift_range=0.1,
            
            height_shift_range=0.1,
            shear_range=2.0,
            zoom_range=0.1,
            channel_shift_range=0,
            
            fill_mode='nearest',
            cval=0.,
            horizontal_flip=True,
            vertical_flip=False,
            
            rescale=None,
            
            preprocessing_function = get_random_eraser(p=0.5, s_l=0.02, s_h=0.1, r_1=0.3, r_2=1/0.3, v_l=0, v_h=0),
            
            validation_split=0.0)

# モデルを作成
model = CNN(input_shape=(32,32,3), class_num=10)

# モデルをコンパイル 
model.compile( Adam(lr=1e-3), loss="categorical_crossentropy", metrics=["acc"])

model.summary()

from keras.callbacks import LearningRateScheduler

#schedulerの設定
def step_decay(epoch):
    x = 0.002
    if epoch >= 30: x = 0.0008
    if epoch >= 40: x = 0.0001
    return x
decay = LearningRateScheduler(step_decay, verbose=1)
#datagen.fit(X_train)
#validationgen.fit(x_test)

#データ増大と学習率の変更をしながら学習
history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=128), 
                              steps_per_epoch=len(x_train) // 128, epochs=50, validation_data=(x_test,y_test), callbacks=[decay])

import matplotlib.pyplot as plt

train_acc = history.history['acc']
test_acc = history.history['val_acc']
x = np.arange(len(train_acc))
plt.plot(x, train_acc, label = 'train accuracy')
plt.plot(x, test_acc, label = 'test accuracy')
plt.legend()